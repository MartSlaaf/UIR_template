% Класс документов по ГОСТ 7.32-2001 "Отчёт о научно-исследовательской работе"
% на основе ГОСТ 2.105-95
% Автор - Алексей Томин, с помощью списка рассылки latex-gost-request@ice.ru,
%  "extreport.cls", "lastpage.sty" и конференции RU.TEX
% Лицензия GPL
% Все вопросы, замечания и пожелания сюда: mailto:alxt@yandex.ru
% Дальнейшая разработка и поддержка - Михаил Конник,
% связаться можно по адресу mydebianblog@gmail.com
% Подгонка проекта под реалии 17-й кафедры - Жаров Ярослав. 
% Связь через mart.slaaf@gmail.com или через github - MartSlaaf

\documentclass[utf8,usehyperref,14pt]{G7-32}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc} %% ваша любимая кодировка здесь
\usepackage[english,russian]{babel} %% это необходимо для включения переносов
\usepackage{float}
\usepackage{graphicx} % работа с графикой
\usepackage{listings} % для возможности вставки исходников
\usepackage{cmap} % pdf должен быть копируемым, для проверки на цитирование
\graphicspath{{pictures/}}

\TableInChaper % таблицы будут нумероваться в пределах р1аздела
\PicInChaper   % рисунки будут нумероваться в пределах раздела
\setlength\GostItemGap{2mm}% для красоты можно менять от 0мм

% Определяем заголовки для титульной страницы

\NirManager{}{Катаев Д.Е.} %% {научное звание и должность}{ФИО} - руководителя

%\NirYear{2014}%% если нужно поменять год отчёта; если закомментировано, ставится текущий год
\NirTown{Москва} %% город, в котором написан отчёт

\NirStudentGroup{K8-171} % группа
\NirStudent{Жаров Я.М.} % ФИО

\bibliographystyle{unsrt} %Стиль библиографических ссылок БибТеХа

%%%%%%%<------------- НАЧАЛО ДОКУМЕНТА
\begin{document}
\usefont{T2A}{ftm}{m}{} %%% Использование шрифтов Т2 для возможности скопировать текст из PDF-файлов.

\frontmatter %%% <-- это выключает нумерацию ВСЕГО; здесь начинаются ненумерованные главы типа Исполнители, Обозначения и прочее

\NirTitle{\textbf{«Исследование особенностей обучения вейвлет-нейронных сетей и вейвнетов»}} % тема УИРа

%\Referat
% Отчет \totalpages~с, \totaltables~таблица, \totalfigures~рисунок, \totalbibs~источник.

\tableofcontents


\Introduction
Вейвлет-нейронные сети и вейвнеты это развитие и соединение двух областей: теории вейвлет-преобразования и нейронных сетей. Имея между собой некоторые различия эти виды сетей, однако, имеют общую основу - это искусственная нейронная сеть, активационными функциями в которых служат вейвлеты, порожденные от общей материнской функции. В вейвлет-нейронных сетях изменениям в процессе обучения подвергаются только веса, в то время как в вейвнете изменяются так же коэффициенты активационной функции. Данные виды сетей способны быть устойчивыми к зашумлению входного сигнала \cite{Veitch}. В обучении вейвлет-нейронных сетей и вейвнетов существует несколько особенностей, связанных, в основном, с начальной инициализацие параметров. Целью данной работы является анализ, проверка и, по возможности, обобщение методов проведения инициализации и обучения вейвлет-нейронных сетей и вейвнетов.

\mainmatter %% это включает нумерацию глав и секций в документе ниже
\chapter{Обзор известных аппаратов вейвлетов и нейронных сетей}
\section{Вейвлет-анализ}
Вейвлет - функция с компактным носителем. Вейвлет преобразование было разработано как замена Фурье-преобразованию, показавшему свою ограниченность при поиске закономерностей в некоторых сигналах. Вейвлет преобразование, в отличие от Фурье, переводит сигнал из временной области, в область вейвлет-коэффициентов - сдвиг $ \cdot $ сжатие $ \cdot $ уровень (или время $ \cdot $ масштаб $ \cdot $ уровень). При этом сдвиг и сжатие - коэффициенты порождающие конкретный вейвлет из материнского, по формуле \eqref{daughter_wavelet}, а уровень показывает насколько получившийся вейвлет подходит к сигналу. Таким образом мы получаем изображение, дающее нам понятие, как о частотных, так и о временных характеристиках сигнала пример на рисунке \ref{gauss_cwt}.
\begin{equation}
\psi_{s,\tau}(t) = \frac{1}{\sqrt{s}} \psi \left(( \frac{t-\tau}{s} \right)
\label{daughter_wavelet}
\end{equation}
\begin{figure}[H]
 \centering{
 	\includegraphics[width=0.6\textwidth]{gauss_CWT.png}
  }
  \caption{Пример результата вейвлет-анализа}\label{gauss_cwt}
\end{figure}
 
\section{Искуственные нейронные сети}
Искусственная нейронная сеть представляет из себя	систему соединенных и взимодействующих между собой простых процессоров. Каждый из таких процессоров имеет дело только с сигналами, которые он периодически получает от других процессоров и отправляет другим процессорам.И, тем не менее, будучи соединёнными в достаточно большую сеть с управляемым взаимодействием, такие локально простые процессоры вместе способны выполнять довольно сложные задачи. Интересующие нас ИНС с прямым распространением являются сетями, содержащими один или более скрытых слоев с полной связью. Пример топологии такой сети, приведен на изображении \ref{ANN}. Активационная функция нейрона - это функция,которой подвергается входной сигнал, поступающий на этот нейрон. Как правило, веса связей задаются малыми случайными значениями, и изменяются в процессе обучения.
\begin{figure}[H]
 \centering{
 	\includegraphics[width=0.4\textwidth]{Ins4.png}
  }
  \caption{Искуственная нейронная сеть с одним скрытым слоем}\label{ANN}
\end{figure}
Одним из наиболее популярных и простых алгоритмов обучения нейронной сети является алгоритм обратного распространения ошибки. Оющая идея состоит в распространении ошибки от выходов ИНС к ее входам, в направлении обратном распространению сигналов.

\section{Вейвлет-нейронные сети}
Вейвлет-нейронная сеть представляет из себя ИНС, в качестве активационной функции нейронов скрытого слоя которой выступают вейвлеты порожденные от одной материнской функции. Так как вейвлеты обладают компактным носителем это позволяет конкретному нейрону реагировать не  только на конкретный набор входов но и только в отдельном диапазоне. Это позволяет получить некоторую дополнительную помехоустойчивость без дополнительных средств, выходящих за рамки искусственной нейронной сети. Для обучения методом обратного распространения ошибки важным свойством является дифференциируемость передаточной функции, поэтому некоторые вейвлеты не слишком хорошо подходят на роль передаточных функций.

\section{Вейвнеты}
Вейвнеты отличаются от вейвлет-нейронных сетей тем, что кроме весов связей, в процессе обучения могут изменяться также коэффициенты при конкретном вейвлете. Это позволяет получить дополнительную гибкость в процессе обучения, а так же уменьшить требуемую избыточность скрытого слоя. Однако, с другой стороны, это требует усложнения вычисления весов, относительно предыдущего случая.

\section{Постановка задачи учебно-исследовательской работы}
Основной задачей являлась разработка действующей модели вейвлет-нейронной сети и вейвнета, а так же подбор методов инициализации их сравнения, и, по возможности, обобщения. 
Вторичной задачей считалось по возможности дополнить код библиотеки PyBrain.



\chapter{Разработка и реализация моделей вейвлет-нейронных сетей и вейвнетов}
\section{Известные методы инициализации вейвлет-нейронных сетей и вейвнетов}
Самым базовым методом инициализации вейвлет-нейронных сетей и вейвнетов является простая инициализация случайными значениями со случайной материнской функцией. При поиске существующих улучшений этого метода нашлись следующие варианты:
\begin{enumerate}
\item подбор материнской функции под форму предполагаемого сигнала, что позволяет для отдельного абстрактного участка сигнала использовать меньше аппроксимирующих значений;
\item увеличение, относительно рассчетного по формуле Арнольда – Колмогорова – Хехт-Нильсена, позволяет уменьшить вероятность отсутствия вейвлета в какой-либо из ключевых точек;
\item использование данных о диапазоне входных значений, что позволяет более точно размещать экземпляры вейвлетов по параметру смещения;
\item использование данных о частотных характеристиках сигнала в простом виде (учет частоты Найквиста), это уточнение позволяет ограничить с одной стороны параметр сжатия;
\item Фурье анализ сигнала - позволяет ограничить параметр сжатия с двух сторон.
\end{enumerate}
Подобные улучшения положительно влияют на скорость сходимости соответствующих видов ИНС. В основном они разработаны для решения задачи аппроксимации функции, однако при некоторых условиях их можно применять и в других задачах.

\section{Разработка моделей вейвлет-нейронных сетей и вейвнетов}
\subsection{Алгоритм обратного распространения ошибки}
В случае вейвлет-нейронной сети и вейвнета вектор выходных значений $ \vec{y} $ формируется из элементов, рассчитываемых по формуле \eqref{outvec}. 
\begin{equation}
y_{k} = [\sum_{j=1}^{n}\psi(\frac{(\sum_{i=1}^{m}u_{i}\omega_{ij})-t_{j}}{\lambda_{j}})\mu_{jk}]+\chi_{k}
\label{outvec}
\end{equation}
,где $ \psi $ - материнская вейвлет-функция, $ u_{i} $ - i-й входной сигнал, $ \omega_{ij} $ - вес связи между i-м входом и j-м вейвлоном, $ t_{j}, \lambda{j} $ - коэффициенты переноса и сжатия j-го вейвлона, $ \mu_{jk} $ - вес связи между j-м вейвлоном и k-м выходом, $ \chi_{k} $ - добавочный вес для k-го выхода.
Исходя из этого производные для параметров должны рассчитываться по формулам:\\
$ \frac{\partial{y_{k}}}{\partial{\chi_{k}}} = 1 $\\
$ \frac{\partial{y_{k}}}{\partial{\mu_{jk}}} = \psi(\frac{(\sum_{i=1}^{m}u_{i}\omega_{ij})-t_{j}}{\lambda_{j}}) $\\
$ \frac{\partial{y_{k}}}{\partial{\omega_{ij}}} = \psi'(\frac{(\sum_{i=1}^{m}u_{i}\omega_{ij})-t_{j}}{\lambda_{j}})\mu_{jk}\frac{u_{i}}{\lambda_{j}} $\\
$ \frac{\partial{y_{k}}}{\partial{t_{j}}} = -\psi'(\frac{(\sum_{i=1}^{m}u_{i}\omega_{ij})-t_{j}}{\lambda_{j}})\frac{\mu_{jk}}{\lambda_{j}} $\\
$ \frac{\partial{y_{k}}}{\partial{\lambda_{j}}} = -\psi'(\frac{(\sum_{i=1}^{m}u_{i}\omega_{ij})-t_{j}}{\lambda_{j}})\mu_{jk}\frac{\sum_{i=1}^{m}(u_{i}\omega_{ij})-t_{j}}{\lambda_{j}^{2}} $\\
Для удобства вычисления эти функции переводились в матричные выражения:\\
$ \delta X = Err $\\
$ \delta M = Z^{T}.Err $\\
$ \delta \Omega = ((M.Err^{T})^{T}\frac{Z'}{\Lambda})^{T}.U $\\
$ \delta T = Err^{T}.M\frac{Z'}{\Lambda} $\\
$ \delta \Lambda = Z'\frac{U.\Omega-T}{\Lambda\Lambda} $\\
, где $ Err $ - вектор ошибки, a $ Z, Z' $ - вектора результатов вычисления активационной функции скрытого слоя и их производных соответственно. Операция . - матричное умножение, остальные действия производятся поэлементно.

\subsection{Разработка методов автоматической и ручной инициализации}
Описанные методы улучшения инициализирущей процедуры должны применяться с оглядкой на то, что они разработаны для архитектуры сети, подразумевающей одномерные вейвлетные функции, и, как следствие, имеющие один вход и один выход. Однако с некоторыми оговорками их можно попробовать обобщить на приведенный выше случай с суммированием. Для демонстрации того, как влияют границы смещения и сжатия было сделано несколько тестов с разными вариантами инициализации по этим параметрам, при правильной инициализации по другим параметрам.
\begin{figure}[h]
\begin{minipage}[h]{0.5\linewidth}
\center{\includegraphics[width=1\linewidth]{lack_diapa.png} \\ а)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.5\linewidth}
\center{\includegraphics[width=1\linewidth]{norm_diapa.png} \\ б)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.5\linewidth}
\center{\includegraphics[width=1\linewidth]{surplus_diapa.png} \\ в)}
\end{minipage}
\caption{На рисунке приведены примеры вывода при недостаточном (а), нормальном(б) и избыточном(в)  диапазонах смещений }
\label{diapazon_test}
\end{figure}

\subsection{Программная реализация}
Так как в случае с вейвлет-нейронными сетями общая структура библиотеки PyBrain позволяла легко вписать необходимый функционал, без модифицирования ключевых моментов библиотеки, было решено дописывать данный функционал к это библиотеке, в виде отдельного модуля - вейвлона, который может использовать вейвлет в качестве передаточной функции. Так же был разработан предобработчик класса, который позволяет генерировать разные варианты класса для разных алгоритмов инициализации.

В случае с вейвнет не было возможности разрабатывать этот функционал, как часть библиотеки PyBrain, так как в основной архитектуре библиотеки не было заложено возможности обучать какие-либо коэффициэнты в нейронах. В силу этого, для проверки ситуации с вейвнетом был разработан отдельный модуль на языке python 2.7, однако функция генерации разнообразных вариантов класса была сохранена, для возможности дальнейшей интеграции в библиотеку.


\chapter{Тестирование разработанной системы}
\section{Используемые тестовые сценарии}
\subsection{Нахождение оценки функции}
Нахождение оценки функции - одна из основных задач, применяемых к вейвлет-нейронным сетям и вейвнетам. Для проверки работоспособности модуля для этих задач было сымитировано множественное измерение некоторых параметров, с наложенным на них мультипликативным и импульсным шумом. Общий вид представлен на рисунке \ref{sampling}.
\begin{figure}[H]
 \centering{
 	\includegraphics[width=0.9\textwidth]{setting_1.png}
  }
  \caption{Входные данные для первого тестового сценария}\label{sampling}
\end{figure}
\subsection{Прогноз авторегрессии}
В качестве экспериментальных данных для проверки способности модуля к прогнозу авторегрессии были взяты отнормированные на диапазон [0,1] данные о цене акций Google за 10 дней. общий вид функции изображен на рисунке \ref{google}.
\begin{figure}[H]
 \centering{
 	\includegraphics[width=0.9\textwidth]{setting_2.png}
  }
  \caption{Нормированные данные о цене акций Google}\label{google}
\end{figure}
\subsection{Нелинейная система}
Для данного тестового сценария использовалась нелинейная система с зашумленными входами. Общий вид системы представлен на рисунке \ref{nonlinear}. Черным цветом на графике окрашены выходные значения системы.
\begin{figure}[H]
 \centering{
 	\includegraphics[width=0.9\textwidth]{setting_3.png}
  }
  \caption{Данные для нелинейной системы}\label{nonlinear}
\end{figure}
\section{Экспериментальная проверка работоспособности системы}
В ходе тестирования системы преследовались три основных цели:
\begin{enumerate}
\item Проверить работоспособность модуля на основных типах задач, которые ставятся перед подобными системами;
\item Выяснить способность модуля решать задачи с нелинейными системами, без предварительной обработки;
\item Сравнить разнообразные алгоритмы инициализации по скорости работы, и ошибке.
\end{enumerate}
\subsection{Работоспособность модуля для нахождения оценки функции}
Для проверки работоспособности модуля в целях нахождения оценки функции к обоим вариантам модуля (вейвлет-нейронная сеть и вейвнет) был применен первый тестовый сценарий. В результате выполнения этого сценария погрешности составили 0.15\% - вейвлет-нейронная сеть и 0.08\% - вейвнет. Пример вывода для сетей показан на рисунке \ref{test_1_result}.
\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=1\linewidth]{1_w.png} \\ а)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=1\linewidth]{1_wn.png} \\ б)}
\end{minipage}
\caption{Исходная функция, и ее приближение вейвнетом (а) и вейвлет-нейронной сетью (б)}
\label{test_1_result}
\end{figure}
\subsection{Работоспособность модуля для прогнозирования авторегрессии}
Данный тест проводился с использованием второго тестового сценария. В результате эксперимента погрешности составили 0.009\% для обеих сетей. Примеры вывода для сетей показаны на рисунке \ref{test_2_result}.
\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=1\linewidth]{2_w.png} \\ а)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=1\linewidth]{2_wn.png} \\ б)}
\end{minipage}
\caption{Исходная функция, и ее приближение вейвнетом (а) и вейвлет-нейронной сетью (б)}
\label{test_2_result}
\end{figure}
\subsection{Работоспособность модуля для прогнозирования нелинейной системы}
Данная часть тестов применялась исключительно, как эксперимент, на тему того, что сможет сделать ненастроенная система, при сложном входе, за ограниченное время. Результаты оказались достаточно предсказуемыми. За 100 итераций вейвнет успел начать приспосабливаться, в то время, как вейвлет-нейронная сеть переместила случайные колебания к среднему значению выходной функции. Результаты представлены на рисунке \ref{test_3_result}
\begin{figure}[H]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=1\linewidth]{3_w.png} \\ а)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=1\linewidth]{3_wn.png} \\ б)}
\end{minipage}
\caption{Исходная функция, и ее приближение вейвнетом (а) и вейвлет-нейронной сетью (б)}
\label{test_3_result}
\end{figure}
\subsection{Сравнение методов инициализации по погрешности}
Для сравнения методов инициализации набор сетей, инициализированный каждым из методов был запущен на короткое обучение. Результирующая погрешность была усреднена среди всех экземпляров инициированных одинаковым образом.
\begin{longtable}{|p{0.49\linewidth}|p{0.2\linewidth}|p{0.23\linewidth}|}
\multicolumn{3}{l}{\tablename~\ref{Pogo} ~Сравнение методов инициализации по погрешности \label{Pogo}}\\
\hline
Эксперимент & Вейвнет & Вейвлет-нейронная сеть \\
\hline
\endfirsthead
\multicolumn{3}{l}{Продолжение таблицы~\ref{Pogo}}\\
\hline
Эксперимент & Вейвнет & Вейвлет-нейронная сеть   \\
\hline
\endhead
Инициализирование полностью параметрами по умолчанию & 28.9 & 28.8  \\ \hline
Изменение материнского вейвлета с Морле на мексиканскую шляпу & 8.7 & 78 \\ \hline
Инициализирование со случайными параметрами интервала [-1000,1000] и периода [1,1000] с разбросом & 10.4 & 9.7 \\ \hline
Увеличение скрытого слоя вдвое, относительно рассчетного & 21 & 51.4  \\ \hline
Настроенная инициализация интервала & 10.8 & 16.3  \\ \hline
Инициализация частотного параметра по периоду & 12.2 & 10.4  \\ \hline
Инициализация частотного параметра по результатам Фурье-анализа & 11.4 & 10.7  \\ \hline
Увеличение скрытого слоя вдвое, относительно рассчетного & 17 & 82.1  \\ \hline

\end{longtable}

% endof %

\backmatter %% Здесь заканчивается нумерованная часть документа и начинаютяс заключение и ссылки

\Conclusion % заключение к отчёту

В результате данной работы была разработана и реализована новая версия программного обеспечения для оптимизации обучающих выборок с помощью генетического алгоритма. В ходе разработки были проведены модификации алгоритмов некоторых модулей системы, позволившие достигнуть улучшения работы системы по сравнению с предыдущей версией. В процессе реализации был использован набор сторонних библиотек, что позволило упростить дальнейшее модифицирование системы и уменьшить объем реализуемого исходного кода (PyBrain для работы с нейронными сетями и SciPy + NumPy для вычислительных задач). Был проведен переход системы на другой язык программирования (Python 2.7) и использование системы контроля версий (git) для упрощения дальнейшей модификации и поддержания системы.

В результате тестирования разработанной системы были получены следующие новые результаты:
\begin{enumerate}
\item Система показала работоспособность в случае нелинейных систем. То есть способность найти субоптимальное решение такое, что прогноз укладывается в доверительный интервал $ \pm 0.3 $ с вероятностью 97\% для исследованной системы;
\item Модификация алгоритма для устранения избыточности позволяет снизить прирост размера экземпляров в два раза;
\item Модернизация алгоритма мутации способна ускорить сходимость и, по крайней мере в некоторых случаях, улучшить результаты поиска.
\end{enumerate}


\begin{thebibliography}{1} %% здесь библиографический список

\bibitem{Veitch}
{Veitch} D.
\newblock Wavelet Neural Networks and their application in the study of dynamical system.
\newblock Departament of Mathematics University of York. 2005


\bibitem{Clever_Algorithms}
{Brownlee} J.
\newblock Clever Algorithms: Nature-Inspired Programming Recipes.
\newblock ISBN: 978-1-4467-8506-5, 2012. 438 c.

\bibitem{Neural_Networks}
{Комашинский} В.И., {Смирнов} Д.А.
\newblock К63 Нейронные сети и их применение в системах управления и связи.
\newblock М.: Горячая линия-Телеком, 2003. 94 c. ISBN 5-93517-094-9

\bibitem{AI}
{Жданов} А.А.
\newblock Автономный искусственный интеллект 2-е изд.
\newblock М.: БИНОМ. Лаборатория знаний, 2009. 359 c. ISBN 978-5-94774-995-3

\bibitem{SelfSystems}
{Николенко} С.И., {Тулупьев} А.Л.
\newblock Н63 Самообучающиеся системы.
\newblock М.: МЦНМО, 2009. 288 c. ISBN 978-5-940570-506-1

\bibitem{DataMining}
{Han} J., {Kamber} M., {Pei} J.
\newblock Data mining: concepts and techniques 3-rd ed.
\newblock ISBN 978-0-12-381479-1

\bibitem{GAinClassifier}
{Gabrys} B., {Ruta} D.
\newblock Genetic algorithms in classifier fusion
\newblock // Applied Soft Computing. 2006 \No \ 6 С. 337-347

\bibitem{SciPy}
{Oliphant} T.E.
\newblock Python for Scientific Computing
\newblock // Computing in Science \& Engineering. 2007 \No \ 9 С. 10-20

\end{thebibliography}

\newpage
\appendix
\chapter{Код обратного распространения ошибки}
\label{backprop}
\begin{lstlisting}
        def backup(self, delta_Chi, delta_M, delta_Omega, delta_T=None, delta_Lambda=None):
            def step(x, y, o):
                return x + y * etta + etta * (x - o)
            new = {}
            new['summer'] = step(self.summer, delta_Chi, self.old_data['summer'])
            if self.wavemodeon:
                new['dilations'] = step(self.dilations, delta_Lambda, self.old_data['dilations'])
                new['translations'] = step(self.translations, delta_T, self.old_data['translations'])
            new['inconnections'] = step(self.inconnections, delta_Omega, self.old_data['inconnections'])
            new['outconnections'] = step(self.outconnections, delta_M, self.old_data['outconnections'])
            self.old_data['summer'] = self.summer
            if self.wavemodeon:
                self.old_data['dilations'] = self.dilations
                self.old_data['translations'] = self.translations
            self.old_data['inconnections'] = self.inconnections
            self.old_data['outconnections'] = self.outconnections
            self.summer = new['summer']
            if self.wavemodeon:
                self.dilations = new['dilations']
                self.translations = new['translations']
            self.inconnections = new['inconnections']
            self.outconnections = new['outconnections']

        def backward(self, error, input):
            U = np.reshape(input, (1, self.indim))
            Err = np.reshape(error, (1, self.outdim))
            Z = self._mother.function((np.dot(U, self.inconnections) - self.translations)/self.dilations)
            Zs = self._mother.derivative((np.dot(U, self.inconnections) - self.translations)/self.dilations)
            delta_Chi = Err
            delta_M = np.dot(Z.transpose(), Err)
            # print U.shape
            delta_Omega = np.dot(U.transpose(), (np.dot(self.outconnections, Err.transpose()).transpose()*(Zs/self.dilations)))
            # print Z.shape, Zs.shape, delta_Chi.shape, delta_M.shape, delta_Omega.shape
            if self.wavemodeon:
                delta_T = np.dot(Err, self.outconnections.transpose())*(Zs/self.dilations)
                delta_Lambda = Zs*((np.dot(U, self.inconnections) - self.translations)/(self.dilations*self.dilations))
                self.backup(delta_Chi, delta_M, delta_Omega, delta_T, delta_Lambda)
            else:
                self.backup(delta_Chi, delta_M, delta_Omega)
\end{lstlisting}

% \bibliography{biblio/filosofy} %% вместо вставки библиографии можно использовать базы BiBTeX - просто раскомментируйте эту строку.
\end{document}
